---
# Source: consul/templates/telemetry-collector-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: consul-telemetry-collector
  namespace: test-namespace
  labels:
    app: consul
    release: consul
    component: consul-telemetry-collector

spec:
  type: ClusterIP
  ports:
    - port: 9356
      targetPort: 9356
  selector:
    app: consul
    component: consul-telemetry-collector
---
# Source: consul/templates/telemetry-collector-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: consul-telemetry-collector
  namespace: test-namespace
  labels:
    app: consul
    release: consul
    component: consul-telemetry-collector
---
# Source: consul/templates/telemetry-collector-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: consul-telemetry-collector
  namespace: test-namespace
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: release-name
    component: consul-telemetry-collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app: consul
      chart: consul-helm
      release: release-name
      component: consul-telemetry-collector
  template:
    metadata:
      annotations:
        "consul.hashicorp.com/connect-inject": "false"
        # This annotation tells the endpoints controller that this pod was injected even though it wasn't. The
        # endpoints controller would then sync the endpoint into Consul
        "consul.hashicorp.com/connect-inject-status": "injected"
        # We aren't using tproxy and we don't have an original pod. This would be simpler if we made a path similar
        # to gateways
        "consul.hashicorp.com/connect-service-port": "metricsserver"
        "consul.hashicorp.com/transparent-proxy": "false"
        "consul.hashicorp.com/transparent-proxy-overwrite-probes": "false"
        "consul.hashicorp.com/connect-k8s-version": 1.4.0-dev
        # vault annotations

      labels:
        consul.hashicorp.com/connect-inject-managed-by: consul-k8s-endpoints-controller
        app: consul
        chart: consul-helm
        release: release-name
        component: consul-telemetry-collector
    spec:
      # This needs to explicitly be consul-telemetry-collector because we look this up from each service consul-dataplane
      # to forward metrics to it.
      serviceAccountName: consul-telemetry-collector
      initContainers:
        # We're manually managing this init container instead of using the connect injector so that we don't run into
        # any race conditions on the connect-injector deployment or upgrade
        - name: consul-connect-init
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: CONSUL_NODE_NAME
              value: $(NODE_NAME)-virtual
            - name: CONSUL_ADDRESSES
              value: consul-server.test-namespace.svc
            - name: CONSUL_GRPC_PORT
              value: "8502"
            - name: CONSUL_HTTP_PORT
              value: "8501"
            - name: CONSUL_DATACENTER
              value: dc1
            - name: CONSUL_API_TIMEOUT
              value: 5s
            - name: CONSUL_PARTITION
              value: default
            - name: CONSUL_LOGIN_PARTITION
              value: default
            - name: CONSUL_USE_TLS
              value: "true"
            - name: CONSUL_CACERT_FILE
              value: "/consul/tls/ca/tls.crt"
            - name: CONSUL_TLS_SERVER_NAME
              value: server.dc1.consul
            # acl login info
            - name: CONSUL_LOGIN_AUTH_METHOD
              value: consul-k8s-auth-method
            - name: CONSUL_LOGIN_DATACENTER
              value: dc1
            - name: CONSUL_LOGIN_META
              value: "component=consul-telemetry-collector,pod=$(NAMESPACE)/$(POD_NAME)"
            # service and login namespace
            # this is attempting to replicate the behavior of webhooks in calculating namespace
            # https://github.com/hashicorp/consul-k8s/blob/b84339050bb2c4b62b60cec96275f74952b0ac9d/control-plane/connect-inject/webhook/consul_dataplane_sidecar.go#L200
            - name: CONSUL_NAMESPACE
              value: test-namespace
            - name: CONSUL_LOGIN_NAMESPACE
              value: default
          command:
            - /bin/sh
            - -ec
            - |-
              consul-k8s-control-plane connect-init -pod-name=${POD_NAME} -pod-namespace=${POD_NAMESPACE} \
                -log-level=trace \
                -log-json=false \
                -service-account-name="consul-telemetry-collector" \
                -service-name="" \
                -proxy-id-file="/consul/connect-inject/proxyid"

          image: hashicorp/consul-k8s-control-plane:1.2.2
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 50m
              memory: 150Mi
            requests:
              cpu: 50m
              memory: 25Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - mountPath: /consul/connect-inject
              name: consul-connect-inject-data
            - name: consul-ca-cert
              mountPath: /consul/tls/ca
              readOnly: true
      containers:
        - name: consul-telemetry-collector
          image: hashicorp/consul-telemetry-collector:0.0.2
          imagePullPolicy:
          ports:
            - containerPort: 9090
              name: metrics
              protocol: TCP
            - containerPort: 9356
              name: metricsserver
              protocol: TCP
          env:
            # These are mounted as secrets so that the telemetry-collector can use them when cloud is enabled.
            # - the hcp-go-sdk in consul agent will already look for HCP_CLIENT_ID, HCP_CLIENT_SECRET, HCP_AUTH_URL,
            #   HCP_SCADA_ADDRESS, and HCP_API_HOST.  so nothing more needs to be done.
            # - HCP_RESOURCE_ID is created for use in the global cloud section but we will share it here
            - name: HCP_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: consul-hcp-observability-client-id
                  key: client-id
            - name: HCP_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  name: consul-hcp-observability-client-secret
                  key: client-secret
            - name: HCP_RESOURCE_ID
              valueFrom:
                secretKeyRef:
                  name: consul-hcp-resource-id
                  key: resource-id

          command:
            - "/bin/sh"
            - "-ec"
            - |

              consul-telemetry-collector agent \
          volumeMounts:
          resources:
            limits:
              cpu: 1000m
              memory: 512Mi
            requests:
              cpu: 1000m
              memory: 512Mi
        # consul-dataplane container
        - name: consul-dataplane
          image: "hashicorp/consul-dataplane:1.2.2"
          imagePullPolicy: IfNotPresent
          command:
            - consul-dataplane
          args:
            # addresses
            - -addresses=consul-server.test-namespace.svc
            # grpc
            - -grpc-port=8502
            - -proxy-service-id-path=/consul/connect-inject/proxyid
            # tls
            - -ca-certs=/consul/tls/ca/tls.crt
            - -tls-server-name=server.dc1.consul
            # credentials
            - -credential-type=login
            - -login-bearer-token-path=/var/run/secrets/kubernetes.io/serviceaccount/token
            - -login-auth-method=consul-k8s-auth-method
            # service and login namespace
            - -service-namespace=test-namespace
            - -login-namespace=default
            # service and login partition
            - -service-partition=default
            - -login-partition=default
            # telemetry
            - -telemetry-prom-scrape-path=/metrics
            - -log-level=trace
            - -log-json=false
            - -envoy-concurrency=2
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: DP_CREDENTIAL_LOGIN_META1
              value: pod=$(NAMESPACE)/$(POD_NAME)
            - name: DP_CREDENTIAL_LOGIN_META2
              value: component=consul-telemetry-collector
            - name: DP_SERVICE_NODE_NAME
              value: $(NODE_NAME)-virtual
            - name: TMPDIR
              value: /consul/connect-inject
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 20000
            timeoutSeconds: 1
          securityContext:
            readOnlyRootFilesystem: true
            runAsGroup: 5995
            runAsNonRoot: true
            runAsUser: 5995
          # dataplane volume mounts
          volumeMounts:
            - mountPath: /consul/connect-inject
              name: consul-connect-inject-data
            - name: consul-ca-cert
              mountPath: /consul/tls/ca
              readOnly: true
      volumes:
        - emptyDir:
            medium: Memory
          name: consul-connect-inject-data
        - name: consul-ca-cert
          secret:
            secretName: consul-ca-cert
            items:
              - key: tls.crt
                path: tls.crt
        - name: config
          configMap:
            name: consul-telemetry-collector
